{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models are based on MNIST dataset Image-shape=(28,28,1)\n",
    "# use_bias=False where BN is applied after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "G=Sequential([\n",
    "    Dense(7*7*256, use_bias=False, input_shape=(100,),kernel_initializer=w_init),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    \n",
    "    Reshape((7, 7, 256)),\n",
    "    Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False,kernel_initializer=w_init),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    \n",
    "    Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False,kernel_initializer=w_init),\n",
    "    BatchNormalization(),\n",
    "    ReLU(),\n",
    "    \n",
    "    Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', activation='tanh',kernel_initializer=w_init)\n",
    "],name='Generator')\n",
    "\n",
    "\n",
    "\n",
    "D=Sequential([\n",
    "    Conv2D(64, (5, 5), strides=(2, 2), padding='same',use_bias=False,input_shape=(28,28,1),kernel_initializer=w_init),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(0.2),\n",
    "    \n",
    "    Conv2D(128, (5, 5), strides=(2, 2), padding='same',use_bias=False,kernel_initializer=w_init),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(0.2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(1,kernel_initializer=w_init)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_DCGAN:\n",
    "    def __init__(self,G,D,isGraph=False):\n",
    "        self.G=G\n",
    "        self.D=D\n",
    "        self.isGraph=isGraph\n",
    "        \n",
    "        self.G_optim=Adam(learning_rate=0.0002,beta_1=0.5)\n",
    "        self.D_optim=Adam(learning_rate=0.0002,beta_1=0.5)\n",
    "        \n",
    "        self.noise_size=self.G.input_shape[1]\n",
    "        self.cross_entropy = BinaryCrossentropy(from_logits=True)\n",
    "        \n",
    "        #these params are for plotting\n",
    "        image_shape=self.G.output_shape[1:]\n",
    "        if image_shape[-1]==1:\n",
    "            self.reshape_dim=image_shape[:-1]\n",
    "        else:\n",
    "            self.reshape_dim=image_shape\n",
    "\n",
    "    \n",
    "    def sample_z(self,batch_size):\n",
    "        return tf.random.uniform([batch_size,self.noise_size])\n",
    "        \n",
    "    @tf.function\n",
    "    def generator_loss(self,fake_output):\n",
    "        return self.cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    \n",
    "    @tf.function\n",
    "    def discriminator_loss(self,real_output,fake_output):\n",
    "        real_loss = self.cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        fake_loss = self.cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        total_loss = real_loss + fake_loss\n",
    "        return total_loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_on_batch(self,x):\n",
    "        batch_size=x.shape[0]\n",
    "        z=self.sample_z(batch_size)\n",
    "        \n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "            real_output=self.D(x,training=True)\n",
    "            fake_output=self.D(self.G(z,training=True),training=True)\n",
    "            d_loss=self.discriminator_loss(real_output,fake_output)\n",
    "            g_loss=self.generator_loss(fake_output)\n",
    "        \n",
    "        d_grads=dis_tape.gradient(d_loss,self.D.trainable_variables)\n",
    "        g_grads=gen_tape.gradient(g_loss,self.G.trainable_variables)\n",
    "        \n",
    "        self.D_optim.apply_gradients(zip(d_grads,self.D.trainable_variables))\n",
    "        self.G_optim.apply_gradients(zip(g_grads,self.G.trainable_variables))\n",
    "        \n",
    "        return d_loss,g_loss\n",
    "    \n",
    "    def sample_images(self,number):\n",
    "        z=self.sample_z(number)\n",
    "        return self.G(z,training=False).numpy().reshape(number,*self.reshape_dim)\n",
    "    \n",
    "    def sample_graphs(self,number):\n",
    "        z=self.sample_z(number)\n",
    "        scaled_adjs=self.G(z,training=False).numpy().reshape(number,*self.reshape_dim)\n",
    "        rescale_adj=(scaled_adjs*0.5)+0.5\n",
    "        return np.round(rescale_adj)\n",
    "    \n",
    "    def get_img_of_gen_samples_from_fig(self,figure):\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        plt.close(figure)\n",
    "        buf.seek(0)\n",
    "        image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        return image\n",
    "    \n",
    "    def get_generated_image_grid(self):\n",
    "        gen_images=self.sample_images(25)\n",
    "        figure = plt.figure(figsize=(10,10))\n",
    "        for i in range(25):\n",
    "            # Start next subplot.\n",
    "            plt.subplot(5, 5, i + 1, title=f\"gen:{i}\")\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.grid(False)\n",
    "            plt.imshow(gen_images[i], cmap=plt.cm.binary)\n",
    "        return figure\n",
    "    \n",
    "    def get_generated_graph_grid(self):\n",
    "        gen_images=self.sample_graphs(25)\n",
    "        figure = plt.figure(figsize=(10,10))\n",
    "        for i in range(25):\n",
    "            # Start next subplot.\n",
    "            plt.subplot(5, 5, i + 1, title=f\"gen:{i}\")\n",
    "            network=nx.from_numpy_array(gen_images[i],create_using=nx.DiGraph)\n",
    "            nx.draw(network,node_size=50)\n",
    "        return figure\n",
    "    \n",
    "    def get_GenImg_out_2_Log(self):\n",
    "        if self.isGraph:\n",
    "            return self.get_img_of_gen_samples_from_fig(self.get_generated_graph_grid())\n",
    "        else:\n",
    "            return self.get_img_of_gen_samples_from_fig(self.get_generated_image_grid())\n",
    "    \n",
    "    def Accuracy_stats(self,x):\n",
    "        batch_size=x.shape[0]\n",
    "        z=self.sample_z(batch_size)\n",
    "        real_prob=tf.round(tf.nn.sigmoid(self.D(x,training=False)))\n",
    "        fake_prob=tf.round(tf.nn.sigmoid(self.D(self.G(z,training=False),training=False)))\n",
    "        \n",
    "        dis_real_acc=tf.reduce_mean(tf.cast(tf.equal(tf.ones_like(real_prob),real_prob),dtype=tf.float32))\n",
    "        dis_fake_acc=tf.reduce_mean(tf.cast(tf.equal(tf.zeros_like(fake_prob),fake_prob),dtype=tf.float32))\n",
    "        \n",
    "        dis_acc=(dis_real_acc+dis_fake_acc).numpy()/2\n",
    "        \n",
    "        gen_fake_acc=tf.reduce_mean(tf.cast(tf.equal(tf.ones_like(fake_prob),fake_prob),dtype=tf.float32))\n",
    "        gen_acc=gen_fake_acc.numpy()\n",
    "        \n",
    "        return dis_acc,gen_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    (x_train, y_train), (x_test, y_test)=tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "    return ((x_train- 127.5) / 127.5).reshape(-1,28,28,1)\n",
    "\n",
    "def load_graphs(community_size=28):\n",
    "    with open(f\"./data/true_data_{community_size}.pickle\", 'rb') as handle:\n",
    "        data = pickle.load(handle)['true_data']\n",
    "\n",
    "    graphs=np.array([nx.graphmatrix.adj_matrix(d).toarray() for d in data])\n",
    "    scaled_graphs=((graphs-0.5)/0.5)#*0.95 #label smoothing\n",
    "    graphs_reshaped=scaled_graphs.reshape(-1,community_size,community_size,1)\n",
    "    return graphs_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingVGAN:\n",
    "    def __init__(self,VGAN,dataset,batch_size=128,experiment_name=\"\"):\n",
    "        self.VGAN=VGAN\n",
    "        self.dataset=tf.data.Dataset.from_tensor_slices(dataset).shuffle(dataset.shape[0]).batch(batch_size,\n",
    "                                                                                                 drop_remainder=True)\n",
    "        self.epoch=0\n",
    "        self.num_of_batch=dataset.shape[0]//batch_size\n",
    "        \n",
    "        self.current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.log_dir = 'logs/VGAN/' + self.current_time + experiment_name\n",
    "        self.summary_writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        \n",
    "    def train(self,epochs=50,log_epoch=1):\n",
    "        while self.epoch<=epochs:\n",
    "            D_LOSS=0;G_LOSS=0\n",
    "            D_ACC=0;G_ACC=0\n",
    "            for images in self.dataset:\n",
    "                d_acc,g_acc=self.VGAN.Accuracy_stats(images)\n",
    "                d_loss,g_loss=self.VGAN.train_on_batch(images)\n",
    "                D_LOSS+=d_loss.numpy();G_LOSS+=g_loss.numpy()\n",
    "                D_ACC+=d_acc;G_ACC+=g_acc\n",
    "            D_LOSS/=self.num_of_batch;G_LOSS/=self.num_of_batch\n",
    "            D_ACC/=self.num_of_batch;G_ACC/=self.num_of_batch\n",
    "            \n",
    "            if self.epoch%log_epoch==0:\n",
    "                print(\"ON EPOCH {}\".format(self.epoch))\n",
    "                GEN_IMAGES=self.VGAN.get_GenImg_out_2_Log()\n",
    "                with self.summary_writer.as_default():\n",
    "                    tf.summary.scalar('loss/Generator', G_LOSS, step=self.epoch)\n",
    "                    tf.summary.scalar('loss/Discriminator', D_LOSS, step=self.epoch)\n",
    "                    tf.summary.scalar('acc/Generator', G_ACC, step=self.epoch)\n",
    "                    tf.summary.scalar('acc/Discriminator', D_ACC, step=self.epoch)\n",
    "                    tf.summary.image('Generated_images',GEN_IMAGES,step=self.epoch)\n",
    "                \n",
    "            \n",
    "            self.epoch+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN=Vanilla_DCGAN(G,D,isGraph=False)\n",
    "\n",
    "algo=TrainingVGAN(GAN,load_mnist(),128,\"mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "algo.train(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN=Vanilla_DCGAN(G,D,isGraph=True)\n",
    "\n",
    "algo=TrainingVGAN(GAN,load_graphs(28),128,\"yeast-graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "algo.train(300) # 120 epoch is enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NICE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
